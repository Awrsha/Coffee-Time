# -*- coding: utf-8 -*-
"""Vacuum Cleaner Algorithm in Ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w8uV-6fPBaiybbxh4edxQbGqeCuf9rN3
"""

import gym
from gym import spaces
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from collections import deque
import random
import tensorflow as tf

# Enable GPU acceleration if available
physical_devices = tf.config.list_physical_devices('GPU')
if physical_devices:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)

class VacuumCleanerEnv(gym.Env):
    def __init__(self, grid_size=5, dirt_rate=0.3):
        super(VacuumCleanerEnv, self).__init__()
        self.grid_size = grid_size
        self.dirt_rate = dirt_rate
        self.action_space = spaces.Discrete(5)  # LEFT, RIGHT, UP, DOWN, SUCK
        self.observation_space = spaces.Box(low=0, high=1, shape=(grid_size, grid_size), dtype=np.float32)
        self.grid = np.zeros((grid_size, grid_size))
        self.position = (random.randint(0, grid_size-1), random.randint(0, grid_size-1))
        self.generate_dirt()

    def generate_dirt(self):
        num_dirty_cells = int(self.dirt_rate * self.grid_size * self.grid_size)
        for _ in range(num_dirty_cells):
            x, y = random.randint(0, self.grid_size-1), random.randint(0, self.grid_size-1)
            self.grid[x][y] = 1

    def step(self, action):
        x, y = self.position
        if action == 0 and y > 0:
            self.position = (x, y - 1)
        elif action == 1 and y < self.grid_size - 1:
            self.position = (x, y + 1)
        elif action == 2 and x > 0:
            self.position = (x - 1, y)
        elif action == 3 and x < self.grid_size - 1:
            self.position = (x + 1, y)

        reward = self.suck_dirt()
        done = self.is_done()
        observation = self.grid
        info = {}
        return observation, reward, done, info

    def reset(self):
        self.grid = np.zeros((self.grid_size, self.grid_size))
        self.position = (random.randint(0, self.grid_size-1), random.randint(0, self.grid_size-1))
        self.generate_dirt()
        return self.grid

    def suck_dirt(self):
        x, y = self.position
        if self.grid[x][y] == 1:
            self.grid[x][y] = 0
            return 1  # Reward for cleaning dirt
        return 0  # No reward if no dirt

    def is_done(self):
        return np.sum(self.grid) == 0

class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        self.gamma = 0.95  # discount rate
        self.epsilon = 1.0  # exploration rate
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.001
        self.model = self._build_model()
        self.target_model = self._build_model()
        self.update_target_model()

    def _build_model(self):
        model = Sequential()
        model.add(Dense(24, input_dim=self.state_size, activation='relu'))
        model.add(Dense(24, activation='relu'))
        model.add(Dense(self.action_size, activation='linear'))
        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))
        return model

    def update_target_model(self):
        self.target_model.set_weights(self.model.get_weights())

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)
        act_values = self.model.predict(state)
        return np.argmax(act_values[0])

    def replay(self, batch_size):
        minibatch = random.sample(self.memory, batch_size)
        for state, action, reward, next_state, done in minibatch:
            target = self.model.predict(state)
            if done:
                target[0][action] = reward
            else:
                a = self.model.predict(next_state)[0]
                t = self.target_model.predict(next_state)[0]
                target[0][action] = reward + self.gamma * t[np.argmax(a)]
            self.model.fit(state, target, epochs=1, verbose=0)
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

def main():
    env = VacuumCleanerEnv()
    state_size = env.observation_space.shape[0] * env.observation_space.shape[1]
    action_size = env.action_space.n
    agent = DQNAgent(state_size, action_size)
    episodes = 1000
    batch_size = 32

    for e in range(episodes):
        state = env.reset()
        state = np.reshape(state, [1, state_size])
        for time in range(500):
            action = agent.act(state)
            next_state, reward, done, _ = env.step(action)
            next_state = np.reshape(next_state, [1, state_size])
            agent.remember(state, action, reward, next_state, done)
            state = next_state
            if done:
                agent.update_target_model()
                print("episode: {}/{}, score: {}, e: {:.2}".format(e, episodes, time, agent.epsilon))
                break
            if len(agent.memory) > batch_size:
                agent.replay(batch_size)

if __name__ == "__main__":
    main()